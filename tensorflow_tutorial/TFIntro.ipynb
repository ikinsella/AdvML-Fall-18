{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TFIntro.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "JPSzqLTEif5B",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# A Modern Introduction to Tensorflow (Advanced ML 2018)\n",
        "\n",
        "*\\[Some brief acknowledgments - this tutorial has been adapted from a few tutorials done by <a href=\"https://github.com/wendazhou\">Wenda Zhou</a> (including one created for last year's iteration of this course), so thanks to him!\\]*\n",
        "\n",
        "This notebook is a quick (i.e about 75 minutes) introduction to <a href=\"https://www.tensorflow.org/\">Tensorflow</a>, from the point of view of using Tensorflow via Python notebooks, and in particular doing so via Google's <a href=\"https://colab.research.google.com/\">Colab</a>. Tensorflow is an open source framework for **high performance machine learning** - while you don't need Tensorflow to run a logistic regression (although as we'll see, you can easily do so), you'll need something like it to perform the heavy lifting for the types of models we'll come across in the second half of the course.\n",
        "\n",
        "From a technical standpoint, Tensorflow combines the following:\n",
        "\n",
        "*   the ability to define computations in a symbolic manner and extract derivatives automatically by back-propagation;\n",
        "*   the ability to execute these operations on a variety of hardware, including GPUs and TPUs (GPUs are great for general purpose parallization; TPUs are great at multipling tensors together, which happens *a lot* when training deep networks).\n",
        "\n",
        "The benefit to you is that this allows you to focus on\n",
        "\n",
        "* specifying the model (i.e the architecture of your neural network);\n",
        "* specifying the loss (e.g cross entropy for classifcation, squared loss for regression etc); and\n",
        "* specifying the mode of training (e.g SGD, Adam etc);\n",
        "\n",
        "rather than the technical aspects of the implementation of the training procedure. I'll also briefly introduce a diagnostic tool called <a href=\"https://www.tensorflow.org/guide/summaries_and_tensorboard\">Tensorboard</a>, which will be helpful for examining the progress of the training procedure of your model.\n",
        "\n",
        "## Prerequisites\n",
        "In the tutorial, I'll be assuming \n",
        "\n",
        "* that you're comfortable (or at least, not uncomfortable) with Python 3 - see the <a href=\"https://github.com/ikinsella/AdvML-Fall-18/blob/master/python_tutorial/Tutorial.ipynb\">tutorial from the start of the semester</a> if you'd like a refresher;\n",
        "* that you have an understanding of the content from the 'Neural Networks' section from the first half of the course. (If you don't feel comfortable with it, I highly recommend investing some time now making yourself so, as the next half of the course is going to build on this *a lot*.)\n",
        "\n",
        "I'll also be assuming that you're running the tutorial in Colab. The main benefit of this is that this will not require any installation of Tensorflow. \n",
        "\n",
        "If you want to run this locally, then you will need to <a href=\"https://www.tensorflow.org/install/\">install Tensorflow and Tensorboard yourself</a>. **WARNING: We will not provide ANY assistance if you have issues installing or running Tensorflow locally on your laptop.** (If you are using Colab, the hardware factor in diagnosing e.g bugs in code is removed, and it's suprising how frequently this can be a factor.)"
      ]
    },
    {
      "metadata": {
        "id": "QEWTXK05zfcg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Using Tensorflow in Colab\n",
        "\n",
        "Both Tensorflow and Colab are made by Google; therefore you'd hope that using Tensorflow in Colab is straighforward. As we'll see, other than a few things we'll need to keep in mind, the answer is yes. To begin, note that Tensorflow and Tensorboard already come pre-loaded on the virtual machine we're currently running this notebook on:"
      ]
    },
    {
      "metadata": {
        "id": "UX4wLho24hJh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2552
        },
        "outputId": "8bc65cae-bae4-4933-b5b3-e5a92358e8ee"
      },
      "cell_type": "code",
      "source": [
        "!pip list\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Package                  Version   \n",
            "------------------------ ----------\n",
            "absl-py                  0.5.0     \n",
            "altair                   2.2.2     \n",
            "astor                    0.7.1     \n",
            "beautifulsoup4           4.6.3     \n",
            "bleach                   3.0.2     \n",
            "cachetools               2.1.0     \n",
            "certifi                  2018.10.15\n",
            "chardet                  3.0.4     \n",
            "crcmod                   1.7       \n",
            "cycler                   0.10.0    \n",
            "cymem                    2.0.2     \n",
            "cytoolz                  0.9.0.1   \n",
            "decorator                4.3.0     \n",
            "defusedxml               0.5.0     \n",
            "dill                     0.2.8.2   \n",
            "entrypoints              0.2.3     \n",
            "filelock                 3.0.9     \n",
            "future                   0.16.0    \n",
            "gast                     0.2.0     \n",
            "google-api-core          1.5.0     \n",
            "google-api-python-client 1.6.7     \n",
            "google-auth              1.4.2     \n",
            "google-auth-httplib2     0.0.3     \n",
            "google-auth-oauthlib     0.2.0     \n",
            "google-cloud-bigquery    1.1.0     \n",
            "google-cloud-core        0.28.1    \n",
            "google-cloud-language    1.0.2     \n",
            "google-cloud-storage     1.8.0     \n",
            "google-cloud-translate   1.3.1     \n",
            "google-colab             0.0.1a1   \n",
            "google-resumable-media   0.3.1     \n",
            "googleapis-common-protos 1.5.3     \n",
            "grpcio                   1.15.0    \n",
            "h5py                     2.8.0     \n",
            "httplib2                 0.11.3    \n",
            "idna                     2.6       \n",
            "ipykernel                4.6.1     \n",
            "ipython                  5.5.0     \n",
            "ipython-genutils         0.2.0     \n",
            "Jinja2                   2.10      \n",
            "joblib                   0.12.5    \n",
            "jsonschema               2.6.0     \n",
            "jupyter-client           5.2.3     \n",
            "jupyter-core             4.4.0     \n",
            "Keras                    2.1.6     \n",
            "Keras-Applications       1.0.6     \n",
            "Keras-Preprocessing      1.0.5     \n",
            "Markdown                 3.0.1     \n",
            "MarkupSafe               1.0       \n",
            "matplotlib               2.1.2     \n",
            "mistune                  0.8.4     \n",
            "mpmath                   1.0.0     \n",
            "msgpack                  0.5.6     \n",
            "msgpack-numpy            0.4.3.2   \n",
            "murmurhash               1.0.1     \n",
            "nbconvert                5.4.0     \n",
            "nbformat                 4.4.0     \n",
            "networkx                 2.2       \n",
            "nltk                     3.2.5     \n",
            "notebook                 5.2.2     \n",
            "numpy                    1.14.6    \n",
            "oauth2client             4.1.3     \n",
            "oauthlib                 2.1.0     \n",
            "olefile                  0.46      \n",
            "opencv-python            3.4.3.18  \n",
            "pandas                   0.22.0    \n",
            "pandas-gbq               0.4.1     \n",
            "pandocfilters            1.4.2     \n",
            "patsy                    0.5.0     \n",
            "pexpect                  4.6.0     \n",
            "pickleshare              0.7.5     \n",
            "Pillow                   4.0.0     \n",
            "pip                      18.1      \n",
            "plac                     0.9.6     \n",
            "plotly                   1.12.12   \n",
            "pluggy                   0.8.0     \n",
            "portpicker               1.2.0     \n",
            "preshed                  2.0.1     \n",
            "prompt-toolkit           1.0.15    \n",
            "protobuf                 3.6.1     \n",
            "psutil                   5.4.7     \n",
            "ptyprocess               0.6.0     \n",
            "py                       1.7.0     \n",
            "pyasn1                   0.4.4     \n",
            "pyasn1-modules           0.2.2     \n",
            "Pygments                 2.1.3     \n",
            "pygobject                3.26.1    \n",
            "pymc3                    3.5       \n",
            "pyparsing                2.2.2     \n",
            "pystache                 0.5.4     \n",
            "python-apt               1.6.2     \n",
            "python-dateutil          2.5.3     \n",
            "pytz                     2018.5    \n",
            "PyWavelets               1.0.1     \n",
            "PyYAML                   3.13      \n",
            "pyzmq                    16.0.4    \n",
            "regex                    2018.1.10 \n",
            "requests                 2.18.4    \n",
            "requests-oauthlib        1.0.0     \n",
            "rsa                      4.0       \n",
            "scikit-image             0.13.1    \n",
            "scikit-learn             0.19.2    \n",
            "scipy                    0.19.1    \n",
            "seaborn                  0.7.1     \n",
            "setuptools               40.4.3    \n",
            "simplegeneric            0.8.1     \n",
            "six                      1.11.0    \n",
            "spacy                    2.0.16    \n",
            "statsmodels              0.8.0     \n",
            "sympy                    1.1.1     \n",
            "tensorboard              1.11.0    \n",
            "tensorflow               1.12.0rc1 \n",
            "tensorflow-hub           0.1.1     \n",
            "termcolor                1.1.0     \n",
            "terminado                0.8.1     \n",
            "testpath                 0.4.2     \n",
            "Theano                   1.0.3     \n",
            "thinc                    6.12.0    \n",
            "toml                     0.10.0    \n",
            "toolz                    0.9.0     \n",
            "tornado                  4.5.3     \n",
            "tqdm                     4.27.0    \n",
            "traitlets                4.3.2     \n",
            "typing                   3.6.6     \n",
            "ujson                    1.35      \n",
            "uritemplate              3.0.0     \n",
            "urllib3                  1.22      \n",
            "vega-datasets            0.5.0     \n",
            "virtualenv               16.0.0    \n",
            "wcwidth                  0.1.7     \n",
            "webencodings             0.5.1     \n",
            "Werkzeug                 0.14.1    \n",
            "wheel                    0.32.1    \n",
            "wrapt                    1.10.11   \n",
            "xgboost                  0.7.post4 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "s5Jn2tG26ZNm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Now that we know that Tensorflow is installed, we can import it using `import tensorflow as tf` and proceed."
      ]
    },
    {
      "metadata": {
        "id": "cST6eXtd68Jb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Basics of Tensorflow: Dataflow graphs\n",
        "\n",
        "In Tensorflow, everything* lives in a graph. This shouldn't suprise you too much - neural networks were introduced to you via a graphical representation as a good way of thinking about them conceptually, and this perspective is beneficial from a computational perspective too. However, this means we'll need to write our scripts/programs in a slightly different way then what you'll be used to using vanilla Python.\n",
        "\n",
        "\\* For now, at least. I'm intentionally ignoring \"eager execution\" in TF, where the 'graph building' will be hidden behind the scenes. Although future versions of Tensorflow (from 2.0) will default to eager execution, it's not completely there yet."
      ]
    },
    {
      "metadata": {
        "id": "2O4KEdRi6ehc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## A motivating example\n",
        "\n",
        "I'll introduce the difference via an example. Let us suppose that we were interested in sampling a single $\\mathcal{N}(3, 4)$ random variable via Tensorflow. In regular Python, we could just use NumPy and write"
      ]
    },
    {
      "metadata": {
        "id": "PMi_R4yu9VXb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3228b96f-068b-4410-c559-85e9be0eeeca"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "a = 2.0*np.random.normal() + 3.0\n",
        "print(a)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5.074186798694685\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r-Ieihno9krt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "which is simple enough. However, in Tensorflow, when we say that everything lives in a graph, we really do mean **everything**. If we just try and naively repeat what we did above, except using the Tensorflow analogues of e.g `np.random.normal`, we get the following:"
      ]
    },
    {
      "metadata": {
        "id": "2yaHG3P990WW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "31e560bd-559b-462e-d2d3-a30580234293"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "a_tf = 2.0*tf.random_normal(shape=[]) + 3.0\n",
        "print(a_tf)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"add:0\", shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dlX1Ck7S-Wgv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Rather than getting a numerical value, we're instead getting something called a `Tensor` object (more on those later). So, what is happening here? In e.g *NumPy*, variables store values and *operators execute operations*:"
      ]
    },
    {
      "metadata": {
        "id": "6odUVDuoE44Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cf454d7c-1c88-49bd-d7ec-1d81aba22459"
      },
      "cell_type": "code",
      "source": [
        "a = 2*np.random.normal()    # a is a variable, * and np.random.normal() \n",
        "                            # are operators\n",
        "b = np.array([3])           # b is a variable, np.array() is a operator\n",
        "c = np.add(a, b)            # c is a variable storing the result of the \n",
        "                            # execution of np.add on a and b\n",
        "\n",
        "print(c)                    # Note that as c is a variable, printing it twice\n",
        "print(c)                    # gives the same thing twice"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1.42015094]\n",
            "[1.42015094]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ibX1ef_HFmC2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "whereas **in Tensorflow**, **operators define the result of an operation,** and we must **explicitly** ask Tensorflow to perform the computation:"
      ]
    },
    {
      "metadata": {
        "id": "by_u0jV-if6A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "a9e7c615-dd22-4014-8b20-ff976cd25f7e"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()\n",
        "a_tf = tf.multiply(2.0, tf.random_normal(shape=[])) # a_tf stores the op of\n",
        "                                                    # sampling a N(0, 4)\n",
        "b_tf = tf.constant(3.0)    # b_tf stores the op of creating a node = 3\n",
        "c_tf = tf.add(a_tf, b_tf)  # c_tf stores the op of a_tf + b_t\n",
        "\n",
        "print(c_tf)                   # Thus printing c_tf gives us a Tensor object...\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  print(sess.run(c_tf))       # and then we only get the result after evaluation\n",
        "  print(sess.run(c_tf))       # Evaluating again gives us a different result!"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensor(\"Add:0\", shape=(), dtype=float32)\n",
            "3.4947917\n",
            "8.961396\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1BNf7SgK22O4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## The default graph"
      ]
    },
    {
      "metadata": {
        "id": "dKOSq8x42LjO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that in the above code, we haven't specified a graph - we've simply started building it on a global graph created by default by Tensorflow. This can be potentially undesirable; for example, if you run the following code block several times, you'll see that the output changes several times. "
      ]
    },
    {
      "metadata": {
        "id": "sSiBXTzZ34g9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "8034cd94-0e86-4af9-b033-6eaaab4d6e14"
      },
      "cell_type": "code",
      "source": [
        "x = tf.constant(4, name='a')\n",
        "x"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'a:0' shape=() dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "xeYcu7Ff38V9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is because a constant named `a` already exists; as the name must be unique, the name gets changed. Another example (which ends up causing an error) is the following code. Run this twice, and compare what happens the first and second time you run it."
      ]
    },
    {
      "metadata": {
        "id": "SOfqweo249Ll",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x = tf.get_variable('a', initializer=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8BUmeibx4-Wu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Because of this, it is recommended to explicitly set a default graph via the use of a `with` block; this helps to avoid some otherwise hard to diagnose errors like what occurs above:"
      ]
    },
    {
      "metadata": {
        "id": "snhZQs2m7fAR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "7ad7de48-a7bf-46bd-d202-2fe8368d45a1"
      },
      "cell_type": "code",
      "source": [
        "# Note that running this command severl times doesn't change what the output is\n",
        "with tf.Graph().as_default():\n",
        "    x = tf.constant(4, name='a')\n",
        "x"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor 'a:0' shape=() dtype=int32>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "uw8q_1M_70Zp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is also possible to reset the current graph via `tf.reset_default_graph`, or by restarting the kernel. If you have a bug in your code and you suspect it has something to do with the graph construction process, the latter is usually a good first step in trying to debug."
      ]
    },
    {
      "metadata": {
        "id": "M3kNKMh278BO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Executing your code - sessions"
      ]
    },
    {
      "metadata": {
        "id": "tDPl2Rm078d5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In order to execute your code, Tensorflow uses the notion of a `tf.Session` in order to execute the computations you've described when building a graph. (The 'behind the hood' reason for this is that Tensorflow needs to manage the resources allocated to the computation, which becomes very important when you begin involving the GPU as part of performing calculations.)\n",
        " \n",
        "In most cases (in particular, ones you will come across in this course), it is best to cleanly seperate the definition of the computation (i.e building the graph) to it's execution. This can be done in the following way:"
      ]
    },
    {
      "metadata": {
        "id": "vrjYPPYj9VP7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "88da35de-917a-4a48-e35d-920aff4ab108"
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    x = tf.constant('Hello world')     # We build the graph within a 'with' \n",
        "                                       # block as discussed before\n",
        "    with tf.Session() as sess:\n",
        "        print(sess.run(x))       # We then tell Tensorflow to begin computing"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'Hello world'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nGmYjOJO9kWS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "It is worth remembering that each graph may only correspond to one session, and moreover we must create the session with the default graph being in scope. This means that the following code won't work, as the session is being run outside of the scope of the default graph:"
      ]
    },
    {
      "metadata": {
        "id": "AKegoOd--8IW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    x = tf.constant('Hello world')\n",
        "    \n",
        "with tf.Session() as sess:\n",
        "    print(sess.run(x)) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eDAEggJF_CXj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you'll likely be spending all of your time in a notebook, there is also a notion of a `tf.InteractiveSession` which is useful. (The only difference is that an `InteractiveSession` installs itself as the default session, meaning we can call e.g `x.run()` without having to pass a specific `Session` object. )"
      ]
    },
    {
      "metadata": {
        "id": "OiZMcG-5-tYX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Further information\n",
        "\n",
        "If you want more detail as to the specifics of graphs in Tensorflow and what is happening under the hood, I highly recommend looking at the <a href=\"https://www.tensorflow.org/guide/graphs\">Graphs and Sessions</a> section on the Tensorflow website."
      ]
    },
    {
      "metadata": {
        "id": "FPL_NbXoAObN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Basics of Tensorflow: Tensors\n",
        "\n",
        "As hinted by the name, Tensorflow is designed to compute on **tensors**. (I don't know where the 'flow' part of the name comes from, however.) As this isn't a math course, for us tensors are simply multidimensional arrays of numerical data. If you're used to Numpy, then these are basically the Tensorflow version of `np.ndarry`. Tensors are characterized by two properties:\n",
        "\n",
        "*   a **datatype** - this is usually `tf.float32` or `tf.int32` (note: these have a `tf.` prefix!)\n",
        "*   the **shape** - this could be e.g $10 \\times 4$, $10 \\times 20 \\times 30$ etc.\n",
        "\n",
        "Let's try and contextualize this. Consider the following image taken from the ImageNet dataset:\n",
        "\n",
        "![A picture of a panda from ImageNet. Sorry it isn't showing for you.](https://www.tensorflow.org/images/cropped_panda.jpg)\n",
        "\n",
        "How can we store this as a tensor? (Note: If we want our neural networks to try and do computer vision tasks such as image recognition, we're going to need to be able to do this.) As the image is $100$ pixels by $100$ pixels, we can store this as a $100 \\times 100 \\times 3$ tensor:\n",
        "\n",
        "* the first two dimensions of the array correspond to a $(x, y)$ coordinate of a pixel;\n",
        "* the last dimension corresponds to the **R**ed, **G**reen and **B**lue channels of the image.\n",
        "\n",
        "In terms of the datatype, this could be either `tf.int32` (for example, if the image is 24 bit - so there are 8 bits per channel - then the entries will be an integer between $0$ and $255 = 2^8 - 1$) or a `tf.float32` (if we have images of different bit-depths, we will want to normalize them to lie in a range $[0, 1]$ so that our model is agnostic to this). \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nrvOIUq-VjCH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Important types of tensors\n",
        "\n",
        "In Tensorflow, there are several types of tensors which you will come across. The two main ones to keep in mind are:\n",
        "\n",
        "*   `tf.Variable` - Variables can be assigned values, and importantly these will be remembered across **several calls** to `session.run`. This will be used for the weights and biases in the network. (Why? If you think of one call of `session.run` as corresponding to one step of the optimization algorithm used to train your neural network, then you'd like to remember the weights/biases after each step!)\n",
        "*   `tf.placeholder` - Placeholders are given values only at runtime (when `session.run` is called), which will allow us to pass arguments to the Tensorflow operations we are about to use. For example, we can then use this to pass the training data into our model so we can then try and fit it.\n",
        "\n",
        "We can see the difference between the two by running the two code blocks below:"
      ]
    },
    {
      "metadata": {
        "id": "oswRexkIrv2n",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "32b4e46d-c6e7-41a8-fcb4-58771972f073"
      },
      "cell_type": "code",
      "source": [
        "# tf.Variable example\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    # Create a variable with initial value 4\n",
        "    a = tf.get_variable('variable_1', initializer = 4)\n",
        "    \n",
        "    # This operation represents subtracting one from a and then reassigning\n",
        "    # it to a. Note that it does not run it!\n",
        "    subtract_a = tf.assign(a, a - 1)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        # Note that variables must be initialized before they are used!\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        print(sess.run(a))\n",
        "        sess.run(subtract_a)\n",
        "        print(sess.run(a))\n",
        "        sess.run(subtract_a)\n",
        "        print(sess.run(a))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n",
            "3\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6DCNkQrRHyR4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "f33a033e-d762-4bec-8c1d-86a6339f794b"
      },
      "cell_type": "code",
      "source": [
        "# tf.placeholder example\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    # We need to define the shape of the placeholder.\n",
        "    # Use none to indicate that the extent is unknown\n",
        "    a = tf.placeholder(tf.float32, shape=[None])\n",
        "    \n",
        "    sum_a = tf.reduce_sum(a)\n",
        "    \n",
        "    with tf.Session() as sess:\n",
        "        # We provide the data when running using the feed_dict\n",
        "        print(sess.run(sum_a, feed_dict={a: [1, 2, 3]}))\n",
        "        print(sess.run(sum_a, feed_dict={a: [4, 5, 6]}))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "6.0\n",
            "15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rB2594xQGJ6N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Operations on tensors\n",
        "\n",
        "Tensorflow provides several pre-defined operations on tensors - pointwise arithmetic, tensor inner products, convolutions, pretty much anything you'd want. One thing worth noting is that, much like e.g NumPy, Tensorflow overloads the normal Python operators such as `+`, and so you should be careful of the exact semantics of the overload if you do use them."
      ]
    },
    {
      "metadata": {
        "id": "NEk7xmZyt-Nk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 128
        },
        "outputId": "fff366ec-5c68-4e7d-dfac-c4e46478b1bf"
      },
      "cell_type": "code",
      "source": [
        "tf.reset_default_graph()      \n",
        "a = tf.constant([[0, 1], [2,3]], dtype=tf.float32)\n",
        "b = tf.constant([[1, 2], [3, 4]], dtype=tf.float32)\n",
        "\n",
        "matmul = tf.matmul(a, b)         # Multiplying two matrices\n",
        "matadd = tf.add(a, b)            # Adding two tensors together\n",
        "matadd_od = a + b                # Adding two tensors together, but using + \n",
        "                                 # (this is overloaded by Tensorflow)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    print(sess.run(matmul))\n",
        "    print(sess.run(matadd))       \n",
        "    print(sess.run(matadd_od))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 3.  4.]\n",
            " [11. 16.]]\n",
            "[[1. 3.]\n",
            " [5. 7.]]\n",
            "[[1. 3.]\n",
            " [5. 7.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-2gLscUfCfBB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# An example: Logistic Regression\n",
        "\n",
        "As promised earlier, here we'll go through an example of using Tensorflow to perform a simple logistic regression in order to classify handwriting digits. By doing so, we'll identify the key APIs which will end up being helpful for you to end up building and fitting models to a given dataset. \n",
        "\n",
        "(Note: The use of these APIs is most useful for when it comes to your projects - don't worry if this is a bit confusing at first! Your homeworks will be styled in a 'fill in this section of code' fashion to help you get used to what is happening in Tensorflow - remember that the point of this course is to teach you about machine learning, not programming.)"
      ]
    },
    {
      "metadata": {
        "id": "8W63ZDh5XKv-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Digit recognition on MNIST\n",
        "\n",
        "In any postal/package delivery company (e.g USPS, UPS, FedEx), they will frequently recieve letters and parcels whose delivery addresses are **handwritten**. Now, 50 years ago these would be sorted by hand, and for the standards of 50 years ago, this did a good job. However, nowadays \n",
        "\n",
        "* the expected timeframe for deliveries is far shorter (within a few days), and\n",
        "* the scale of distribution is far greater (both in terms of volume and the possible places packages can be sent to in the world),\n",
        "\n",
        "and so we need an automated solution in order to read handwriting and figure out where we should send the letter to. We also want our automated solution to have a lower error rate.\n",
        "\n",
        "A simplified version of this problem is to simply infer handwritten digits, and the classical example of a dataset which consists of handwritten digits is the [MNIST dataset](http://yann.lecun.com/exdb/mnist/). The method of collection isn't too important for now - all which we need to know is that\n",
        "\n",
        "* there are 50,000 training examples\n",
        "* there are 10,000 test examples\n",
        "* each handwritten digit is given by a 28px by 28px greyscale image, along with a classification label from 0 to 9.\n",
        "\n",
        "As you'll see soon in the course, it's far better to use e.g a convolutional neural network on this type of task. For now though at least, we'll just fit a simple logistic classifier to see how to do so in Tensorflow. "
      ]
    },
    {
      "metadata": {
        "id": "IoSJVhhrGpx5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Loading data: the dataset API\n",
        "\n",
        "The dataset API (`tf.data`) is the main method - and also the best practice way - for loading data into Tensorflow. It works with simple text files, binary files, and tfrecord files (think of the latter as a special proprietary format for importing data into Tensorflow). You can also directly load data from NumPy arrays, but this does not scale well. \n",
        "\n",
        "One useful thing to note is that there are pre-existing files or conversion utilities for the MNIST, CIFAR and Imagenet datasets. I'll go through [some example code provided for the MNIST example](https://github.com/tensorflow/models/blob/master/official/mnist/dataset.py), so then we can see what is happening under the hood.\n",
        "\n",
        "We'll begin by downloading the data. Note that if you look under \"Files > sample_data\" you'll come across a smaller version of the MNIST dataset. However, it'll be good to know how to import other data sets from online - remember that we're working on a Google VM, not locally!"
      ]
    },
    {
      "metadata": {
        "id": "3iYFL8Jd10c5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "33c066cb-05b0-427f-c692-454cce7ddf7c"
      },
      "cell_type": "code",
      "source": [
        "# Downloading the data - most of this can be ignored for our purposes\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "from six.moves import urllib\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def read32(bytestream):\n",
        "  \"\"\"Read 4 bytes from bytestream as an unsigned 32-bit integer.\"\"\"\n",
        "  dt = np.dtype(np.uint32).newbyteorder('>')\n",
        "  return np.frombuffer(bytestream.read(4), dtype=dt)[0]\n",
        "\n",
        "\n",
        "def check_image_file_header(filename):\n",
        "  \"\"\"Validate that filename corresponds to images for the MNIST dataset.\"\"\"\n",
        "  with tf.gfile.Open(filename, 'rb') as f:\n",
        "    magic = read32(f)\n",
        "    read32(f)  # num_images, unused\n",
        "    rows = read32(f)\n",
        "    cols = read32(f)\n",
        "    if magic != 2051:\n",
        "      raise ValueError('Invalid magic number %d in MNIST file %s' % (magic,\n",
        "                                                                     f.name))\n",
        "    if rows != 28 or cols != 28:\n",
        "      raise ValueError(\n",
        "          'Invalid MNIST file %s: Expected 28x28 images, found %dx%d' %\n",
        "          (f.name, rows, cols))\n",
        "\n",
        "\n",
        "def check_labels_file_header(filename):\n",
        "  \"\"\"Validate that filename corresponds to labels for the MNIST dataset.\"\"\"\n",
        "  with tf.gfile.Open(filename, 'rb') as f:\n",
        "    magic = read32(f)\n",
        "    read32(f)  # num_items, unused\n",
        "    if magic != 2049:\n",
        "      raise ValueError('Invalid magic number %d in MNIST file %s' % (magic,\n",
        "                                                                     f.name))\n",
        "\n",
        "\n",
        "def download(directory, filename):\n",
        "  \"\"\"Download (and unzip) a file from the MNIST dataset if not already done.\"\"\"\n",
        "  filepath = os.path.join(directory, filename)\n",
        "  if tf.gfile.Exists(filepath):\n",
        "    return filepath\n",
        "  if not tf.gfile.Exists(directory):\n",
        "    tf.gfile.MakeDirs(directory)\n",
        "  # CVDF mirror of http://yann.lecun.com/exdb/mnist/\n",
        "  url = 'https://storage.googleapis.com/cvdf-datasets/mnist/' + filename + '.gz'\n",
        "  _, zipped_filepath = tempfile.mkstemp(suffix='.gz')\n",
        "  print('Downloading %s to %s' % (url, zipped_filepath))\n",
        "  urllib.request.urlretrieve(url, zipped_filepath)\n",
        "  with gzip.open(zipped_filepath, 'rb') as f_in, \\\n",
        "      tf.gfile.Open(filepath, 'wb') as f_out:\n",
        "    shutil.copyfileobj(f_in, f_out)\n",
        "  os.remove(zipped_filepath)\n",
        "  return filepath\n",
        "\n",
        "download('data/', 'train-images-idx3-ubyte')\n",
        "download('data/', 'train-labels-idx1-ubyte')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-images-idx3-ubyte.gz to /tmp/tmp0jiquyet.gz\n",
            "Downloading https://storage.googleapis.com/cvdf-datasets/mnist/train-labels-idx1-ubyte.gz to /tmp/tmpj_zdjds2.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'data/train-labels-idx1-ubyte'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "metadata": {
        "id": "AagtZF0m5YdQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Note that this was relatively painless! You might need to be careful if your session times out and Colab clears it out, but otherwise there is nothing to worry about - in particular, as the code below shows, we don't need to refer to it by however it is saved in the `/tmp/` directory. \n",
        "\n",
        "Let's have a quick look at the dataset and look through it:"
      ]
    },
    {
      "metadata": {
        "id": "sYQFSiKQ5WMC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "424a868d-3656-433c-9e91-0b4332bb73de"
      },
      "cell_type": "code",
      "source": [
        "with tf.Graph().as_default():\n",
        "    dataset = tf.data.FixedLengthRecordDataset('data/train-images-idx3-ubyte', \n",
        "                                               28 * 28, header_bytes=16)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    next_sample = iterator.get_next()\n",
        "    \n",
        "    with tf.Session() as session:\n",
        "        print(session.run(next_sample))\n",
        "        print(len(session.run(next_sample)))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b\"\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x03\\x12\\x12\\x12~\\x88\\xaf\\x1a\\xa6\\xff\\xf7\\x7f\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1e$^\\x9a\\xaa\\xfd\\xfd\\xfd\\xfd\\xfd\\xe1\\xac\\xfd\\xf2\\xc3@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x001\\xee\\xfd\\xfd\\xfd\\xfd\\xfd\\xfd\\xfd\\xfd\\xfb]RR8'\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\xdb\\xfd\\xfd\\xfd\\xfd\\xfd\\xc6\\xb6\\xf7\\xf1\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00P\\x9ck\\xfd\\xfd\\xcd\\x0b\\x00+\\x9a\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0e\\x01\\x9a\\xfdZ\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x8b\\xfd\\xbe\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x0b\\xbe\\xfdF\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00#\\xf1\\xe1\\xa0l\\x01\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00Q\\xf0\\xfd\\xfdw\\x19\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00-\\xba\\xfd\\xfd\\x96\\x1b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x10]\\xfc\\xfd\\xbb\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xf9\\xfd\\xf9@\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00.\\x82\\xb7\\xfd\\xfd\\xcf\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00'\\x94\\xe5\\xfd\\xfd\\xfd\\xfa\\xb6\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x18r\\xdd\\xfd\\xfd\\xfd\\xfd\\xc9N\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x17B\\xd5\\xfd\\xfd\\xfd\\xfd\\xc6Q\\x02\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x12\\xab\\xdb\\xfd\\xfd\\xfd\\xfd\\xc3P\\t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x007\\xac\\xe2\\xfd\\xfd\\xfd\\xfd\\xf4\\x85\\x0b\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x88\\xfd\\xfd\\xfd\\xd4\\x87\\x84\\x10\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x00\"\n",
            "784\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ABjoHEkz5eDP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Currently it's a sequence of bytes, which isn't the most interesting thing in the world. In the case of a logistic regression, we could keep the data as being arranged in this fashion, but for more general models (particularly CNNs) we'll want to convert it to a $28 \\times 28 \\times 1$ tensor which is more natural:"
      ]
    },
    {
      "metadata": {
        "id": "H3BKPa8e6M0F",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 677
        },
        "outputId": "0eaa501c-43a4-4537-d262-b4218d111644"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "    # Although our intuition carries across, it is important to always remember\n",
        "    # that we are working with symbolic computations. In particular, this\n",
        "    # function, which describes an op that will be executed on each image,\n",
        "    # is only called once.\n",
        "    def _format_image(raw_data):\n",
        "        image = tf.decode_raw(raw_data, tf.uint8)\n",
        "        image = tf.to_float(image)\n",
        "        image = tf.reshape(image, [28, 28])\n",
        "        image = image / 255\n",
        "        return image\n",
        "    \n",
        "    dataset = tf.data.FixedLengthRecordDataset('data/train-images-idx3-ubyte', \n",
        "                                               28 * 28, header_bytes=16)\n",
        "    dataset = dataset.map(_format_image)\n",
        "    iterator = dataset.make_one_shot_iterator()\n",
        "    next_sample = iterator.get_next()\n",
        "    \n",
        "    with tf.Session() as session:\n",
        "        plt.imshow(session.run(next_sample))\n",
        "        plt.show()\n",
        "        plt.imshow(session.run(next_sample))\n",
        "        plt.show()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd5e58ff6a0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUsAAAFKCAYAAACU6307AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYZ0Aw5r6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd5e1c69908>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "r-lDngjb6Rwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We'll define this as a function, as it'll come in handy later:"
      ]
    },
    {
      "metadata": {
        "id": "BZkjSiPk6ewh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_mnist_dataset():\n",
        "    \"\"\" This function creates a dataset which can be used to load data \n",
        "    from the MNIST dataset. \"\"\"\n",
        "    def _format_image(raw_data):\n",
        "        image = tf.decode_raw(raw_data, tf.uint8)\n",
        "        image = tf.to_float(image)\n",
        "        image = tf.reshape(image, [28, 28, 1])\n",
        "        image = image / 255\n",
        "        return image\n",
        "    \n",
        "    def _format_label(raw_data):\n",
        "        label = tf.decode_raw(raw_data, tf.uint8)\n",
        "        label = tf.reshape(label, [])\n",
        "        return tf.to_int32(label)\n",
        "    \n",
        "    dataset_img = tf.data.FixedLengthRecordDataset('data/train-images-idx3-ubyte', 28 * 28, header_bytes=16)\n",
        "    dataset_img = dataset_img.map(_format_image)\n",
        "    \n",
        "    dataset_label = tf.data.FixedLengthRecordDataset('data/train-labels-idx1-ubyte', 1, header_bytes=8)\n",
        "    dataset_label = dataset_label.map(_format_label)\n",
        "    \n",
        "    return tf.data.Dataset.zip((dataset_img, dataset_label))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wVV5JqfTHVO8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Building and training your model: the Estimator API\n",
        "\n",
        "The estimator API (`tf.estimator`) is the main method to build and train estimators in Tensorflow. It abstracts numerous details about setting up the tensorflow graph and executing the training, whilst also being flexible enough to be workable for most experiments. One peculiarity of the estimator API is that it abstracts the model building behind a function, so we will be working with functions that return functions.\n",
        "\n",
        "(As an aside, Tensorflow also provides canned estimators - i.e premade estimators - but they are not of interest today.)\n",
        "\n",
        "The estimator API is based off two notions:\n",
        "\n",
        "* an input function used to import the data \n",
        "* a model function used to create the model when it is called.\n",
        "\n",
        "We'll begin by looking at an example of a model function:"
      ]
    },
    {
      "metadata": {
        "id": "z4AHRCrndiTM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode):\n",
        "    # Firstly, begin by noting that the model_fn takes in three arguments:\n",
        "    # - features: these should be the handwriting digits\n",
        "    # - labels: the classification label of the digits\n",
        "    # - mode: this allows us to switch between e.g training and testing\n",
        "\n",
        "    # Here we're building up the model \n",
        "    images = features                            # We begin by taking the images\n",
        "    images = tf.layers.flatten(images)           # Flattens the input tensor \n",
        "                                                 # (except for the batch axis)\n",
        "    logits = tf.layers.dense(images, units=10)   # Creates a dense connected layer \n",
        "    \n",
        "    # This part of the model function keeps track of the accuracy \n",
        "    # as a measure of training progress\n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
        "    \n",
        "    metrics = {'accuracy': accuracy}\n",
        "    \n",
        "    # Here we specify the loss function - note that in our last layer, \n",
        "    # we're still working on the logit scale\n",
        "    # (we don't transform them to probabilities)\n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
        "    \n",
        "    # Here we're specifying the training procedure\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = 0.001)\n",
        "    train_op = optimizer.minimize(loss, \n",
        "                                  global_step=tf.train.get_or_create_global_step())\n",
        "    \n",
        "    # This gives Tensorflow the description of what must be done\n",
        "    # to construct our model.\n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        loss=loss,\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        predictions=predictions,\n",
        "        eval_metric_ops=metrics)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ppf03us4iy7q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As for the input function, as we've already performed most of the work in the data processing this ends up being pretty straightforward:"
      ]
    },
    {
      "metadata": {
        "id": "ZbrWU9yyjVCq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# The main things we are doing here are doing some shuffling, batching, \n",
        "# and performance tweaks. Don't worry too much about this!\n",
        "def input_fn():\n",
        "    dataset = get_mnist_dataset()\n",
        "    # Shuffle the dataset, and repeat as necessary.\n",
        "    dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(1000)) \n",
        "    dataset = dataset.prefetch(128)\n",
        "    dataset = dataset.batch(batch_size=128)\n",
        "    dataset = dataset.prefetch(2)\n",
        "    \n",
        "    return dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EFk60hqCksui",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In particular, now that we've got our input function and our model function, we're able to start training! Below we'll begin fitting the model for 500 iterations of gradient descent:"
      ]
    },
    {
      "metadata": {
        "id": "BMt87165lGq6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "outputId": "ce437da5-40ef-4e23-a826-c086ce88511b"
      },
      "cell_type": "code",
      "source": [
        "# Note that we are passing functions as arguments to the constructor and train\n",
        "# functions, and not their return values!\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn)\n",
        "estimator.train(input_fn, steps=500)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpk9dgbnu7\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpk9dgbnu7', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5e0b7e400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpk9dgbnu7/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4708664, step = 1\n",
            "INFO:tensorflow:global_step/sec: 66.5167\n",
            "INFO:tensorflow:loss = 2.333909, step = 101 (1.505 sec)\n",
            "INFO:tensorflow:global_step/sec: 86.9851\n",
            "INFO:tensorflow:loss = 2.227202, step = 201 (1.154 sec)\n",
            "INFO:tensorflow:global_step/sec: 85.943\n",
            "INFO:tensorflow:loss = 2.1404152, step = 301 (1.163 sec)\n",
            "INFO:tensorflow:global_step/sec: 87.4103\n",
            "INFO:tensorflow:loss = 2.082309, step = 401 (1.141 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpk9dgbnu7/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.9596854.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.estimator.estimator.Estimator at 0x7fd5e0b7ed68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "metadata": {
        "id": "f3VcTE8ll-yt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "A few things worth pointing out:\n",
        "\n",
        "* By default, Tensorflow will ouput the loss after every 100 iterations to the console. If you're running training procedures for hundreds of thousands of iterations, you'll want a better way of visualizing the progress. (We'll get to this later.)\n",
        "* Note that Tensorflow actually has estimators work from a directory. If this isn't specified, then this goes to a `/tmp/` directory - good luck accessing this on Colab! The next example will have us specify a directory to show that this is possible in Colab. \n",
        "* At the end of training, we're given the final loss, but not the classification rate (which is what we really care about in classification problems). With some minor changes, we can output this at the end. \n",
        "\n",
        "Lets make some changes to our code to address these last two issues:"
      ]
    },
    {
      "metadata": {
        "id": "CboR2ytkl8f_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "0b2a4651-a8ce-4146-e47d-aabfc8c3340d"
      },
      "cell_type": "code",
      "source": [
        "# Note: if we want to evaluate our estimator, we must modify the input function\n",
        "# for that: we don't want to repeat the dataset forever when evaluating!\n",
        "\n",
        "# We now want a parametrised input function. So we will create a function \n",
        "# that creates a function.\n",
        "def make_input_fn(repeat_count=None, shuffle_size=1000):\n",
        "    def input_fn():\n",
        "        dataset = get_mnist_dataset()\n",
        "        \n",
        "        # Shuffle the dataset, and repeat as necessary\n",
        "        if shuffle_size is not None and shuffle_size > 0:\n",
        "            dataset = dataset.apply(tf.data.experimental.shuffle_and_repeat(shuffle_size, repeat_count))\n",
        "        else:\n",
        "            dataset = dataset.repeat(repeat_count)\n",
        "            \n",
        "        dataset = dataset.prefetch(128)\n",
        "        dataset = dataset.batch(batch_size=128)\n",
        "        dataset = dataset.prefetch(2)\n",
        "    \n",
        "        return dataset\n",
        "    return input_fn\n",
        "\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn,\n",
        "                                   model_dir='models/')\n",
        "print('\\n---- Starting training ------')\n",
        "estimator.train(make_input_fn(), steps=500)\n",
        "print('---- Training Done ------')\n",
        "print('\\n---- Starting evaluation ------')\n",
        "estimator.evaluate(input_fn=make_input_fn(1, 0))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'models/', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5e218c0b8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "\n",
            "---- Starting training ------\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into models/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4661884, step = 1\n",
            "INFO:tensorflow:global_step/sec: 85.9517\n",
            "INFO:tensorflow:loss = 2.3940685, step = 101 (1.165 sec)\n",
            "INFO:tensorflow:global_step/sec: 86.9852\n",
            "INFO:tensorflow:loss = 2.2657123, step = 201 (1.150 sec)\n",
            "INFO:tensorflow:global_step/sec: 84.7569\n",
            "INFO:tensorflow:loss = 2.1380446, step = 301 (1.180 sec)\n",
            "INFO:tensorflow:global_step/sec: 87.0429\n",
            "INFO:tensorflow:loss = 2.0666714, step = 401 (1.155 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into models/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 1.9650408.\n",
            "---- Training Done ------\n",
            "\n",
            "---- Starting evaluation ------\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Starting evaluation at 2018-10-23-11:53:28\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Restoring parameters from models/model.ckpt-500\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Finished evaluation at 2018-10-23-11:53:33\n",
            "INFO:tensorflow:Saving dict for global step 500: accuracy = 0.37548333, global_step = 500, loss = 1.962226\n",
            "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 500: models/model.ckpt-500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'accuracy': 0.37548333, 'global_step': 500, 'loss': 1.962226}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "ebTOMlRCpUCZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "As you can see from our above attempts, running 500 iterations of (stochastic) gradient descent isn't giving us a good model. If we ran it for a bit longer, we'd get something more reasonable. There is also the possibility that the learning rate (or step size) chosen was too small. If we want to experiment to try and find a good learning rate, we will want this to be an adjustable parameter of our network (so we don't need to constantly redefine a network and copy/paste loads of code). To do so, Tensorflow allows a construction to pass parameters to the model function:"
      ]
    },
    {
      "metadata": {
        "id": "7PXdLoMarHtT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "48dda79b-5da1-4f8e-9e09-5acf23724424"
      },
      "cell_type": "code",
      "source": [
        "def model_fn(features, labels, mode, params):\n",
        "    # Here, the params parameter is passed in from Tensorflow\n",
        "    images = features\n",
        "    images = tf.layers.flatten(images)\n",
        "    logits = tf.layers.dense(images, units=10)\n",
        "    \n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
        "    \n",
        "    metrics = {'accuracy': accuracy}\n",
        "    \n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=params['learning_rate'])\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        loss=loss,\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        predictions=predictions,\n",
        "        eval_metric_ops=metrics)\n",
        "\n",
        "# Note that we pass in a dictionary params into the model function via a \n",
        "# separate argument in the tf.estimator.Estimator function\n",
        "estimator = tf.estimator.Estimator(model_fn=model_fn, params={'learning_rate': 0.5})\n",
        "print('\\n---- Starting training ------')\n",
        "estimator.train(make_input_fn(), steps=500)\n",
        "print('---- Training Done ------')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpezaucyf2\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpezaucyf2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5e1f6b390>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "\n",
            "---- Starting training ------\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpezaucyf2/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4488735, step = 1\n",
            "INFO:tensorflow:global_step/sec: 84.6655\n",
            "INFO:tensorflow:loss = 0.4045675, step = 101 (1.184 sec)\n",
            "INFO:tensorflow:global_step/sec: 87.4583\n",
            "INFO:tensorflow:loss = 0.5817108, step = 201 (1.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 84.2686\n",
            "INFO:tensorflow:loss = 0.55518186, step = 301 (1.183 sec)\n",
            "INFO:tensorflow:global_step/sec: 85.2418\n",
            "INFO:tensorflow:loss = 0.48893243, step = 401 (1.171 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpezaucyf2/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.2272084.\n",
            "---- Training Done ------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LXSLRRFDrqBd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "If we wanted to do something more involved - for example, by specifying a decaying learning rate - then remember that we cannot create it before passing it to the estimator, as it will not be part of the same graph! Thankfully, we can get around this by passing in a function as part of our `params` dictionary:"
      ]
    },
    {
      "metadata": {
        "id": "vQQIUyWsr5ac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "728721de-5bd3-4414-c1de-3a9013628d74"
      },
      "cell_type": "code",
      "source": [
        "# This part of code will allow us to easily switch between function/constant \n",
        "# inputs for our learning rate - consider this a small quality of life tool.\n",
        "def _evaluate(fn_or_value):\n",
        "    if callable(fn_or_value):\n",
        "        return fn_or_value()\n",
        "    else:\n",
        "        return fn_or_value\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    images = features\n",
        "    images = tf.layers.flatten(images)\n",
        "    logits = tf.layers.dense(images, units=10)\n",
        "    \n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
        "    \n",
        "    metrics = {'accuracy': accuracy}\n",
        "    \n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
        "    \n",
        "    optimizer = tf.train.GradientDescentOptimizer(\n",
        "        learning_rate=_evaluate(params['learning_rate']))\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        loss=loss,\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        predictions=predictions,\n",
        "        eval_metric_ops=metrics)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn,\n",
        "    params={\n",
        "        'learning_rate': lambda: tf.train.inverse_time_decay(\n",
        "            0.1, tf.train.get_or_create_global_step(),\n",
        "            decay_steps=10,\n",
        "            decay_rate=1,\n",
        "            staircase=True)\n",
        "    })\n",
        "print('\\n---- Starting training ------')\n",
        "estimator.train(make_input_fn(), steps=500)\n",
        "print('---- Training Done ------')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpfe57ji9d\n",
            "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpfe57ji9d', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5e141c9e8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "\n",
            "---- Starting training ------\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into /tmp/tmpfe57ji9d/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4057784, step = 1\n",
            "INFO:tensorflow:global_step/sec: 86.3008\n",
            "INFO:tensorflow:loss = 0.9713109, step = 101 (1.161 sec)\n",
            "INFO:tensorflow:global_step/sec: 84.5019\n",
            "INFO:tensorflow:loss = 1.0264194, step = 201 (1.192 sec)\n",
            "INFO:tensorflow:global_step/sec: 86.3286\n",
            "INFO:tensorflow:loss = 0.87684095, step = 301 (1.149 sec)\n",
            "INFO:tensorflow:global_step/sec: 87.3151\n",
            "INFO:tensorflow:loss = 0.95577854, step = 401 (1.146 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into /tmp/tmpfe57ji9d/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.8218277.\n",
            "---- Training Done ------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "jN7DFjIrHQJv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Specifying an architecture: API's in tf.layers\n",
        "\n",
        "As we've seen above, we can implement logistic regression via a two layer neural network in Tensorflow. Let's be honest - so far what we've done is a lot of overkill. However, thankfully due to the `tf.layers` APIs, the change in code to go from logistic regression:"
      ]
    },
    {
      "metadata": {
        "id": "cwiNAI06vS8X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This block isn't intended to be run. Please don't.\n",
        "images = features\n",
        "images = tf.layers.flatten(images)\n",
        "logits = tf.layers.dense(images, units=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C5Iu9IFPvc9-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "to a much more complicated several layer convolutional neural network:"
      ]
    },
    {
      "metadata": {
        "id": "3TKrITbjvtVL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Again, please don't run this block.\n",
        "\n",
        "x = images\n",
        "x = tf.layers.conv2d(x, filters=20, kernel_size=3, strides=1,\n",
        "                     padding='same', data_format=data_format)\n",
        "x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, data_format=data_format)\n",
        "x = tf.layers.conv2d(x, filters=50, kernel_size=3, strides=1,\n",
        "                     padding='same', data_format=data_format)\n",
        "x = tf.layers.max_pooling2d(x, pool_size=2, strides=2, data_format=data_format)\n",
        "x = tf.layers.average_pooling2d(x, pool_size=(7, 7), strides=(1, 1), data_format=data_format)\n",
        "x = tf.layers.flatten(x)\n",
        "x = tf.layers.dense(x, units=100)\n",
        "logits = tf.layers.dense(x, units=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cHiPGbitv8Uz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "is pretty straightforward.\n",
        "\n",
        "There are two main APIs in `tf.layers`:\n",
        "\n",
        "* a functional API where the types of layers are denoted in lowercase - e.g `tf.layers.conv2d` as above; and \n",
        "* a keras-like object API where the layers are denoted in uppercase - e.g `tf.layers.Conv2D`.\n",
        "\n",
        "The functional API is somewhat sleeker, but does not provide access to the variables, whereas the object API does."
      ]
    },
    {
      "metadata": {
        "id": "zvUSoXMWI18y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model diagnostics: the summary API\n",
        "\n",
        "One of the strengths of Tensorflow is **Tensorboard**, a tool which allows us to visualize training. However, we are responsible to indicate the quantities we wish to record as we design our model, through the use of the `tf.summary` API. The summary APIs are able to record numerous different\n",
        "types of data (scalars, histograms, images, audio). However, be aware that recording too much data can\n",
        "signficantly impact performance."
      ]
    },
    {
      "metadata": {
        "id": "zbgqirRt0HQU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 574
        },
        "outputId": "92df7628-b059-4c74-d81e-cbcc44ab84ee"
      },
      "cell_type": "code",
      "source": [
        "def _evaluate(fn_or_value):\n",
        "    if callable(fn_or_value):\n",
        "        return fn_or_value()\n",
        "    else:\n",
        "        return fn_or_value\n",
        "\n",
        "def model_fn(features, labels, mode, params):\n",
        "    images = features\n",
        "    images = tf.layers.flatten(images)\n",
        "    logits = tf.layers.dense(images, units=10)\n",
        "    \n",
        "    tf.summary.histogram('logits', logits) # Record the weights\n",
        "    \n",
        "    predictions = tf.argmax(logits, axis=1, output_type=tf.int32)\n",
        "    accuracy = tf.metrics.accuracy(labels=labels, predictions=predictions)\n",
        "    \n",
        "    metrics = {'accuracy': accuracy}\n",
        "    \n",
        "    loss = tf.losses.sparse_softmax_cross_entropy(labels, logits)\n",
        "    \n",
        "    tf.summary.scalar('loss', loss) # Record the loss\n",
        "    \n",
        "    learning_rate = _evaluate(params['learning_rate'])\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
        "    train_op = optimizer.minimize(loss, global_step=tf.train.get_or_create_global_step())\n",
        "    \n",
        "    tf.summary.scalar('learning_rate', learning_rate) # Record the loss\n",
        "    \n",
        "    return tf.estimator.EstimatorSpec(\n",
        "        loss=loss,\n",
        "        mode=mode,\n",
        "        train_op=train_op,\n",
        "        predictions=predictions,\n",
        "        eval_metric_ops=metrics)\n",
        "\n",
        "estimator = tf.estimator.Estimator(\n",
        "    model_fn=model_fn, model_dir='log/mod',\n",
        "    params={\n",
        "        'learning_rate': lambda: tf.train.inverse_time_decay(\n",
        "            0.1, tf.train.get_or_create_global_step(),\n",
        "            decay_steps=10,\n",
        "            decay_rate=1,\n",
        "            staircase=True)\n",
        "    })\n",
        "print('\\n---- Starting training ------')\n",
        "estimator.train(make_input_fn(), steps=500)\n",
        "print('---- Training Done ------')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using default config.\n",
            "INFO:tensorflow:Using config: {'_model_dir': 'log/mod', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': allow_soft_placement: true\n",
            "graph_options {\n",
            "  rewrite_options {\n",
            "    meta_optimizer_iterations: ONE\n",
            "  }\n",
            "}\n",
            ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7fd5e2452630>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
            "\n",
            "---- Starting training ------\n",
            "INFO:tensorflow:Calling model_fn.\n",
            "INFO:tensorflow:Done calling model_fn.\n",
            "INFO:tensorflow:Create CheckpointSaverHook.\n",
            "INFO:tensorflow:Graph was finalized.\n",
            "INFO:tensorflow:Running local_init_op.\n",
            "INFO:tensorflow:Done running local_init_op.\n",
            "INFO:tensorflow:Saving checkpoints for 0 into log/mod/model.ckpt.\n",
            "INFO:tensorflow:loss = 2.4822793, step = 1\n",
            "INFO:tensorflow:global_step/sec: 67.7772\n",
            "INFO:tensorflow:loss = 1.0822034, step = 101 (1.481 sec)\n",
            "INFO:tensorflow:global_step/sec: 84.2949\n",
            "INFO:tensorflow:loss = 0.9032658, step = 201 (1.182 sec)\n",
            "INFO:tensorflow:global_step/sec: 82.228\n",
            "INFO:tensorflow:loss = 0.8727783, step = 301 (1.224 sec)\n",
            "INFO:tensorflow:global_step/sec: 87.1276\n",
            "INFO:tensorflow:loss = 0.87488496, step = 401 (1.140 sec)\n",
            "INFO:tensorflow:Saving checkpoints for 500 into log/mod/model.ckpt.\n",
            "INFO:tensorflow:Loss for final step: 0.892076.\n",
            "---- Training Done ------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fbK5U7sbE0Aj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Tensorboard in Colab\n",
        "\n",
        "Tensorboard works by reading Tensorflow events files (which keeps track of whatever summary information you tell it to), and then giving you a really nice visual interface to keep track of it all. However, Tensorboard is intended to be run *locally* on the machine where the model is saved. Remember - we're currently on a virtual machine located....somewhere...in one of Google's many data centers. \n",
        "\n",
        "Thankfully, there is a pretty simple workaround for this (thanks to [Stack Overflow](https://stackoverflow.com/questions/47818822/can-i-use-tensorboard-with-google-colab)). The code you need to run is as follows:"
      ]
    },
    {
      "metadata": {
        "id": "lQBrGSH3-czr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Run Tensorflow in the background - note that we specify the log \n",
        "# directory we want to look at\n",
        "LOG_DIR = 'log/mod'\n",
        "get_ipython().system_raw(\n",
        "    'tensorboard --logdir {} --host 0.0.0.0 --port 6006 &'\n",
        "    .format(LOG_DIR)\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7rD2Wqwk2tVd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        },
        "outputId": "e5b22106-de07-4bfd-9a14-2745e5526f0c"
      },
      "cell_type": "code",
      "source": [
        "# Download and unzip ngrok - you will only need to do this once per session\n",
        "! wget https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
        "! unzip ngrok-stable-linux-amd64.zip"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-10-23 11:56:12--  https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip\n",
            "Resolving bin.equinox.io (bin.equinox.io)... 52.44.144.199, 52.55.191.55, 52.3.63.2, ...\n",
            "Connecting to bin.equinox.io (bin.equinox.io)|52.44.144.199|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5363700 (5.1M) [application/octet-stream]\n",
            "Saving to: ‘ngrok-stable-linux-amd64.zip’\n",
            "\n",
            "ngrok-stable-linux- 100%[===================>]   5.11M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2018-10-23 11:56:13 (42.5 MB/s) - ‘ngrok-stable-linux-amd64.zip’ saved [5363700/5363700]\n",
            "\n",
            "Archive:  ngrok-stable-linux-amd64.zip\n",
            "  inflating: ngrok                   \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oSRQyV-e-lbz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Launch the ngrok background process\n",
        "get_ipython().system_raw('./ngrok http 6006 &')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dRxYVwLb-qMU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "c0c38522-b4d8-483a-a207-51ebc46847f4"
      },
      "cell_type": "code",
      "source": [
        "# Get the public URL and be sorted!\n",
        "! curl -s http://localhost:4040/api/tunnels | python3 -c \\\n",
        "    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\""
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "https://51094e05.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wTDVgRET_fHX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "outputId": "2c698018-fa79-4311-d3da-b51457f4371e"
      },
      "cell_type": "code",
      "source": [
        "# Checking if things are running as intended\n",
        "! ps -eaf | grep 6006"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root         352       1  2 11:56 ?        00:00:02 /usr/bin/python2 /usr/local/bin/tensorboard --logdir log/mod --host 0.0.0.0 --port 6006\n",
            "root         362       1  0 11:56 ?        00:00:00 ./ngrok http 6006\n",
            "root         458      59  0 11:57 ?        00:00:00 /bin/bash -c  ps -eaf | grep 6006\n",
            "root         460     458  0 11:57 ?        00:00:00 /bin/bash -c  ps -eaf | grep 6006\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JHfVIEQa_jhm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Kill processes if something goes wrong\n",
        "# NOTE: You will need to manually kill the tensorboard process if you want to change file\n",
        "# change the file directory\n",
        "! kill -9 634"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Yt_TaJKYXB8R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "As a quick recap to end, the key takeaways from this tutorial are:\n",
        "\n",
        "* that in Tensorflow we begin by building a computational graph which we then execute (in contrast to regular Python)\n",
        "* in Tensorflow, we work with tensors, and there are different types of tensors for whether they e.g store the trainable parameters of our network or the training data\n",
        "* there are several APIs available to help us in loading our data, building and training our model, along with how to keep track of useful diagnostic information\n",
        "* we can use Tensorboard (even in Colab) to help visualize e.g the graph of the network, the progress of training etc.\n",
        "\n",
        "A few last words and bits of advice:\n",
        "\n",
        "* Coding in Tensorflow can be frustrating, and the errors it can spit out are you aren't always...illuminating. **Google and StackOverflow will be your friends.**\n",
        "* If you want to know more about how Tensorflow works, there is a [guide](https://www.tensorflow.org/guide/) available on the Tensorflow website which gives a nice breakdown of the APIs and their features. \n",
        "* Tensorflow is still an evolving language, with the 'best practices' changing frequently (although this has begin to stabilize over the past couple of months). Take this into account when looking at tutorials online. \n",
        "* Deep learning is as much of an art (if not more) than a science. **If you want to improve, you should practice.**"
      ]
    }
  ]
}
